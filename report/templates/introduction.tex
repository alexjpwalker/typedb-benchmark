\documentclass{article}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{float}
\graphicspath{ {./plots/} }
\lstset{
basicstyle=\small\ttfamily,
columns=flexible,
breaklines=true
}

\begin{document}

\title{TypeDB Benchmark}
\date{23rd October 2020}
\author{Vaticle Limited}

\maketitle

\begin{abstract}
Here we introduce the benchmarking report and all of the stuff in it.
\end{abstract}

\section{Introduction}
Here we'll talk about how we went about benchmarking TypeDB, and how we drew comparisons with property graph databases.

Our objectives:
1. Profile TypeDB query latency and throughput as scale increases, with parallelism
    - This is for internal use at Vaticle, to ensure that TypeDB and TypeDB Cluster only improve in performance across all queries without any regression
    - Therefore this is integrated into our CI system, Vaticle Factory
2. Use the same infrastructure to profile other databases against the same synthetically generated data
    - This creates like-for-like data models stored in different database softwares.
    - Emulate units of work that resemble a real application

- Machine specs

\subsubsection{Application Benchmark}

We have built a synthetic data generator that grows the data over each iteration. The generator grows and reads the data using predefined queries, with randomisation introduced in:
- which graph nodes are connected together
- the literal values of properties/attributes inserted
- the number of nodes introduced each iteration

This has been achieved

- With queries that we can profile
- With queries that utilise all of the knowledge representation constructs that TypeQL offers
- With queries that require conclusions from TypeQL rules


\section{Overview}
Comparison of db query performance

\begin{figure}[H]
\centering
\includegraphics[width=1.0\textwidth]{overview}
\end{figure}